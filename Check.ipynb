{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436bfb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY= [UR_KEY]\n",
    "#GEMINI_API_KEY =[UR_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c91e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import logging\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38979a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GEMINI ONE\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import logging\n",
    "import re\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Gemini API Configuration\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Load MATH-500 dataset\n",
    "ds = load_dataset(\"HuggingFaceH4/MATH-500\")\n",
    "\n",
    "# Models to Compare\n",
    "models = {\n",
    "    \"Gemini 2.0 Flash\": \"gemini-2.0-flash\",\n",
    "}\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0164d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPEN Router One\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# OpenRouter Client\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=API_KEY,\n",
    ")\n",
    "\n",
    "# Gemini API Configuration\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Load MATH-500 dataset\n",
    "ds = load_dataset(\"HuggingFaceH4/MATH-500\")\n",
    "\n",
    "# Models to Compare\n",
    "models = {\n",
    "    \"LLaMA 3 8B\": \"meta-llama/llama-3-8b-instruct:free\",\n",
    "    \"DeepSeek Chat\": \"deepseek/deepseek-chat:free\",\n",
    "    \"Dolphin 3.0 R1\": \"cognitivecomputations/dolphin3.0-r1-mistral-24b:free\",\n",
    "}\n",
    "\n",
    "#DO one model at time\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1904da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(problem, level=\"unknown\"):\n",
    "    return f\"\"\"Below is a math problem. Solve it step by step and provide the final answer.\n",
    "\n",
    "PROBLEM:\n",
    "{problem}\n",
    "\n",
    "Please take your time to work through this carefully. Do all the steps and reason it properly and give me all the steps.\n",
    "In case of a word problem, where the answer is in a sentence, give the answer in a word and remove the units like inches or cm etc.\n",
    "Give me the final answer as\n",
    "FINAL ANSWER: [your_answer]\n",
    "Make sure this final answer is a numerical value or word  or an expression. In case of an expression or a word, return it in LaTeX format. \"\"\"\n",
    "\n",
    "def clean_latex_answer(answer):\n",
    "    \"\"\"\n",
    "    Clean and normalize LaTeX answers by removing \\boxed{}, \\left, \\right, and inline math markers.\n",
    "    \"\"\"\n",
    "    # Remove all instances of \\boxed{...}\n",
    "    answer = re.sub(r'\\\\boxed\\s*{\\s*(.*?)\\s*}', r'\\1', answer)\n",
    "\n",
    "    # Remove \\left and \\right\n",
    "    answer = re.sub(r'\\\\left\\s*|\\s*\\\\right', '', answer)\n",
    "\n",
    "    # Remove LaTeX inline math markers like \\( ... \\) and $...$\n",
    "    answer = re.sub(r'\\\\\\(|\\\\\\)|\\$', '', answer)\n",
    "\n",
    "    # Remove extra spaces and strip trailing characters\n",
    "    answer = answer.strip().rstrip(\".,:;\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def extract_answer(response):\n",
    "    # Check for common final answer patterns\n",
    "    final_answer_patterns = [\n",
    "        \"Final answer:\", \"FINAL ANSWER:\", \"final answer is\", \"answer:\", \"Answer:\"\n",
    "    ]\n",
    "\n",
    "    for phrase in final_answer_patterns:\n",
    "        if phrase in response:\n",
    "            answer_part = response.split(phrase)[-1].strip()\n",
    "            \n",
    "            # Apply cleanup using regex\n",
    "            answer_part = clean_latex_answer(answer_part)\n",
    "\n",
    "            # Remove brackets\n",
    "            answer_part = answer_part.strip('[]')\n",
    "            decimal_match = re.search(r\"[-+]?\\d*\\.\\d+\", answer_part)\n",
    "            if decimal_match:\n",
    "                return decimal_match.group(0)\n",
    "\n",
    "            # Get the first line or until a period\n",
    "            if \"\\n\" in answer_part:\n",
    "                return answer_part.split(\"\\n\")[0].strip().rstrip(\".\")\n",
    "            elif \".\" in answer_part:\n",
    "                return answer_part.split(\".\")[0].strip()\n",
    "            else:\n",
    "                return answer_part.strip()\n",
    "\n",
    "    # For word problems (name, single word answers)\n",
    "    lines = response.strip().split('\\n')\n",
    "    for line in reversed(lines):\n",
    "        line = line.strip()\n",
    "        if '\"' in line or \"'\" in line:\n",
    "            try:\n",
    "                return line.split('\"')[1] if '\"' in line else line.split(\"'\")[1]\n",
    "            except IndexError:\n",
    "                continue\n",
    "        elif len(line.split()) == 1:\n",
    "            return line\n",
    "\n",
    "    return \"Answer not found\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_answer(answer):\n",
    "    \"\"\"Normalize answer format for consistent comparison.\"\"\"\n",
    "    answer = str(answer).strip()\n",
    "\n",
    "    # Convert LaTeX variants with \\text{} to plain text\n",
    "    if answer.startswith(r\"\\text{\") and answer.endswith(\"}\"):\n",
    "        answer = answer[6:-1]  # Extract the content within \\text{}\n",
    "\n",
    "    # Remove LaTeX block markers\n",
    "    answer = re.sub(r'\\\\\\[|\\]', '', answer)\n",
    "\n",
    "    # Remove ** or other extra symbols\n",
    "    answer = re.sub(r'^\\*+|\\*+$', '', answer)\n",
    "\n",
    "    # Handle LaTeX-like outputs\n",
    "    answer = answer.replace(\"\\\\dfrac\", \"\\\\frac\")\n",
    "    answer = answer.replace(\"\\\\left\", \"\").replace(\"\\\\right\", \"\")\n",
    "    answer = answer.replace(\"^\\\\circ\", \"\").replace(\"^{\\\\circ}\", \"\").replace(\"^°\", \"\")\n",
    "    answer = answer.replace(\"}{\", \"/\").replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "\n",
    "    # Handle LaTeX matrices (pmatrix, bmatrix, etc.)\n",
    "    matrix_pattern = r\"\\\\begin\\{(?:bmatrix|pmatrix|vmatrix|Vmatrix)\\}(.*?)\\\\end\\{(?:bmatrix|pmatrix|vmatrix|Vmatrix)\\}\"\n",
    "    matrix_match = re.search(matrix_pattern, answer, re.DOTALL)\n",
    "\n",
    "    if matrix_match:\n",
    "        matrix_content = matrix_match.group(1)\n",
    "        # Clean up the matrix and format it as a simple vector\n",
    "        matrix_vector = \"[\" + \", \".join(matrix_content.split(\"\\\\\\\\\")) + \"]\"\n",
    "        answer = matrix_vector.strip()\n",
    "\n",
    "    return answer.strip().rstrip(\".,:;\")\n",
    "\n",
    "\n",
    "\n",
    "def check_numeric_equality(val1, val2):\n",
    "    try:\n",
    "        # Handle fractions\n",
    "        if \"/\" in val1 and \"/\" in val2:\n",
    "            try:\n",
    "                v1_num, v1_denom = map(float, val1.split(\"/\"))\n",
    "                v2_num, v2_denom = map(float, val2.split(\"/\"))\n",
    "\n",
    "                # Check for division by zero\n",
    "                if v1_denom == 0 or v2_denom == 0:\n",
    "                    logger.warning(f\"Division by zero detected when comparing {val1} and {val2}\")\n",
    "                    return False\n",
    "\n",
    "                return abs((v1_num / v1_denom) - (v2_num / v2_denom)) < 1e-6\n",
    "            except ValueError as e:\n",
    "                logger.warning(f\"Error parsing fractions {val1} and {val2}: {str(e)}\")\n",
    "                return False\n",
    "\n",
    "        # Handle direct numeric comparison\n",
    "        try:\n",
    "            return abs(float(val1) - float(val2)) < 1e-6\n",
    "        except ValueError as e:\n",
    "            logger.warning(f\"Error converting to float {val1} and {val2}: {str(e)}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in numeric equality check between {val1} and {val2}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def check_angular_equality(val1, val2):\n",
    "    \"\"\"Compare angular values, handling degrees and radians.\"\"\"\n",
    "    try:\n",
    "        # Convert all to radians for comparison\n",
    "        def to_radians(val):\n",
    "            val = val.replace(\"\\\\pi\", \"pi\").replace(\"π\", \"pi\")\n",
    "\n",
    "            # Handle degrees\n",
    "            if \"°\" in val:\n",
    "                try:\n",
    "                    degrees = float(val.replace(\"°\", \"\"))\n",
    "                    return degrees * (3.14159265359 / 180)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            # Handle pi fractions (pi/2, 3pi/4, etc.)\n",
    "            if \"pi\" in val:\n",
    "                try:\n",
    "                    if \"/\" in val:\n",
    "                        parts = val.replace(\"pi\", \"\").split(\"/\")\n",
    "                        if parts[0].strip() == \"\":\n",
    "                            parts[0] = \"1\"\n",
    "                        num = float(parts[0])\n",
    "                        denom = float(parts[1])\n",
    "                        return num * 3.14159265359 / denom\n",
    "                    else:\n",
    "                        multiplier = 1.0\n",
    "                        if val.replace(\"pi\", \"\").strip():\n",
    "                            multiplier = float(val.replace(\"pi\", \"\"))\n",
    "                        return multiplier * 3.14159265359\n",
    "                except (ValueError, ZeroDivisionError):\n",
    "                    pass\n",
    "\n",
    "            # If it's just a number, return it\n",
    "            try:\n",
    "                return float(val)\n",
    "            except ValueError:\n",
    "                return val\n",
    "\n",
    "        # Convert both values to radians\n",
    "        val1_rad = to_radians(val1)\n",
    "        val2_rad = to_radians(val2)\n",
    "\n",
    "        # If conversion failed (returned non-numeric), do string comparison\n",
    "        if isinstance(val1_rad, str) or isinstance(val2_rad, str):\n",
    "            return val1 == val2\n",
    "\n",
    "        # Compare angles in radians, taking periodicity into account\n",
    "        val1_mod = val1_rad % (2 * 3.14159265359)\n",
    "        val2_mod = val2_rad % (2 * 3.14159265359)\n",
    "\n",
    "        return abs(val1_mod - val2_mod) < 1e-4\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in angular equality check: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def check_answer(predicted, correct):\n",
    "    \"\"\"Check if predicted answer matches correct answer with improved normalization.\"\"\"\n",
    "    try:\n",
    "        # Normalize both answers\n",
    "        predicted_norm = normalize_answer(predicted)\n",
    "        correct_norm = normalize_answer(correct)\n",
    "\n",
    "        # Direct string comparison after normalization\n",
    "        if predicted_norm == correct_norm:\n",
    "            return True\n",
    "\n",
    "        # Strip all LaTeX and spaces for strict comparison\n",
    "        predicted_clean = predicted_norm.replace(\"\\\\\", \"\").replace(\" \", \"\").lower()\n",
    "        correct_clean = correct_norm.replace(\"\\\\\", \"\").replace(\" \", \"\").lower()\n",
    "\n",
    "        if predicted_clean == correct_clean:\n",
    "            return True\n",
    "\n",
    "        # Special handling for common cases\n",
    "\n",
    "        # 1. Angular measurements\n",
    "        if \"°\" in predicted_norm or \"°\" in correct_norm or \"pi\" in predicted_norm or \"pi\" in correct_norm:\n",
    "            return check_angular_equality(predicted_norm, correct_norm)\n",
    "\n",
    "        # 2. Numeric equality for numbers and fractions\n",
    "        try:\n",
    "            # Try direct numeric comparison for simple numbers\n",
    "            if predicted_clean.replace(\".\", \"\").isdigit() and correct_clean.replace(\".\", \"\").isdigit():\n",
    "                return abs(float(predicted_clean) - float(correct_clean)) < 1e-6\n",
    "\n",
    "            # Handle fractions\n",
    "            if \"/\" in predicted_clean and \"/\" in correct_clean:\n",
    "                p_parts = predicted_clean.split(\"/\")\n",
    "                c_parts = correct_clean.split(\"/\")\n",
    "\n",
    "                if len(p_parts) == 2 and len(c_parts) == 2:\n",
    "                    p_num, p_denom = float(p_parts[0]), float(p_parts[1])\n",
    "                    c_num, c_denom = float(c_parts[0]), float(c_parts[1])\n",
    "\n",
    "                    # Check for division by zero\n",
    "                    if p_denom == 0 or c_denom == 0:\n",
    "                        return False\n",
    "\n",
    "                    return abs((p_num / p_denom) - (c_num / c_denom)) < 1e-6\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # 3. Coordinate pairs (x,y) or (r,θ)\n",
    "        if \"(\" in predicted_clean and \")\" in predicted_clean and \"(\" in correct_clean and \")\" in correct_clean:\n",
    "            try:\n",
    "                # Extract values inside parentheses\n",
    "                p_coords = predicted_clean.split(\"(\")[1].split(\")\")[0].split(\",\")\n",
    "                c_coords = correct_clean.split(\"(\")[1].split(\")\")[0].split(\",\")\n",
    "\n",
    "                if len(p_coords) == 2 and len(c_coords) == 2:\n",
    "                    # Compare first coordinate\n",
    "                    if p_coords[0] != c_coords[0]:\n",
    "                        try:\n",
    "                            if abs(float(p_coords[0]) - float(c_coords[0])) > 1e-6:\n",
    "                                return False\n",
    "                        except ValueError:\n",
    "                            if p_coords[0] != c_coords[0]:\n",
    "                                return False\n",
    "\n",
    "                    # Compare second coordinate (could be angle)\n",
    "                    if \"pi\" in p_coords[1] or \"pi\" in c_coords[1]:\n",
    "                        return check_angular_equality(p_coords[1], c_coords[1])\n",
    "                    else:\n",
    "                        try:\n",
    "                            return abs(float(p_coords[1]) - float(c_coords[1])) < 1e-6\n",
    "                        except ValueError:\n",
    "                            return p_coords[1] == c_coords[1]\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error comparing coordinates: {str(e)}\")\n",
    "\n",
    "        # If all else fails, return False\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error comparing answers '{predicted}' and '{correct}': {str(e)}\")\n",
    "        return False\n",
    "\n",
    "import time\n",
    "\n",
    "request_count = 0\n",
    "\n",
    "def query_model(model_id, prompt):\n",
    "    global request_count\n",
    "    try:\n",
    "        if model_id == \"gemini-2.0-flash\":\n",
    "            model = genai.GenerativeModel(model_id)\n",
    "            response = model.generate_content(prompt)\n",
    "            result = response.text\n",
    "        else:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model_id,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1,\n",
    "                max_tokens=2048\n",
    "            )\n",
    "            if not completion or not completion.choices:\n",
    "                return \"Error: Empty response from model.\"\n",
    "            result = completion.choices[0].message.content\n",
    "\n",
    "        request_count += 1\n",
    "\n",
    "        # After every 15 requests, wait for 30 seconds\n",
    "        if request_count % 15 == 0:\n",
    "            print(\"Rate limit reached. Pausing for 30 seconds...\")\n",
    "            time.sleep(30)\n",
    "\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error querying model {model_id}: {str(e)}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "def evaluate_models():\n",
    "    results = {model_name: {\"correct\": 0, \"total\": 0, \"responses\": [], \"errors\": []} for model_name in models}\n",
    "    test_data = list(ds[\"test\"])[:250]  #Sample size\n",
    "\n",
    "    print(f\"Evaluating {len(test_data)} samples.\")\n",
    "\n",
    "    for idx, sample in enumerate(tqdm(test_data, desc=\"Evaluating\")):\n",
    "        try:\n",
    "            problem = sample['problem']\n",
    "            correct_answer = sample['answer']\n",
    "            level = sample.get('level', 'unknown')\n",
    "\n",
    "            prompt = create_prompt(problem, level)\n",
    "\n",
    "            for model_name, model_id in models.items():\n",
    "                try:\n",
    "                    response = query_model(model_id, prompt)\n",
    "\n",
    "                    try:\n",
    "                        predicted_answer = extract_answer(response)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Failed to extract answer for problem {idx}: {str(e)}\")\n",
    "                        predicted_answer = \"Error extracting answer\"\n",
    "                        results[model_name]['errors'].append({\n",
    "                            \"problem_idx\": idx,\n",
    "                            \"error_type\": \"extraction_error\",\n",
    "                            \"error_msg\": str(e)\n",
    "                        })\n",
    "\n",
    "                    try:\n",
    "                        is_correct = check_answer(predicted_answer, correct_answer)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Failed to check answer for problem {idx}: {str(e)}\")\n",
    "                        is_correct = False\n",
    "                        results[model_name]['errors'].append({\n",
    "                            \"problem_idx\": idx,\n",
    "                            \"error_type\": \"check_error\",\n",
    "                            \"error_msg\": str(e)\n",
    "                        })\n",
    "\n",
    "                    results[model_name]['total'] += 1\n",
    "                    if is_correct:\n",
    "                        results[model_name]['correct'] += 1\n",
    "\n",
    "                    results[model_name]['responses'].append({\n",
    "                        \"problem\": problem,\n",
    "                        \"level\": level,\n",
    "                        \"correct_answer\": correct_answer,\n",
    "                        \"model_response\": response,\n",
    "                        \"extracted_answer\": predicted_answer,\n",
    "                        \"is_correct\": is_correct\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to process sample {idx} with model {model_name}: {str(e)}\")\n",
    "                    results[model_name]['errors'].append({\n",
    "                        \"problem_idx\": idx,\n",
    "                        \"error_type\": \"processing_error\",\n",
    "                        \"error_msg\": str(e)\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process sample {idx}: {str(e)}\")\n",
    "            for model_name in models:\n",
    "                results[model_name]['errors'].append({\n",
    "                    \"problem_idx\": idx,\n",
    "                    \"error_type\": \"sample_error\",\n",
    "                    \"error_msg\": str(e)\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n",
    "def calculate_metrics(results):\n",
    "    metrics = {}\n",
    "    for model_name, data in results.items():\n",
    "        accuracy = (data['correct'] / data['total'] * 100) if data['total'] > 0 else 0\n",
    "        error_count = len(data['errors'])\n",
    "        metrics[model_name] = {\n",
    "            \"Overall Accuracy\": f\"{accuracy:.2f}%\",\n",
    "            \"Error Count\": error_count,\n",
    "            \"Completed Samples\": data['total'],\n",
    "            \"Errors Per Sample\": f\"{error_count/data['total']:.4f}\" if data['total'] > 0 else \"N/A\"\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "def print_answer_comparison(results):\n",
    "    \"\"\"Print a side-by-side comparison of extracted answers vs correct answers with normalization.\"\"\"\n",
    "    print(\"\\n===== ANSWER COMPARISON =====\")\n",
    "    print(f\"{'Problem #':<8} {'Model':<15} {'Correct?':<8} {'Raw Extracted':<30} {'Normalized':<25} {'Correct Answer':<30}\")\n",
    "    print(\"-\" * 120)\n",
    "\n",
    "    for model_name, data in results.items():\n",
    "        for idx, response_data in enumerate(data['responses']):\n",
    "            correct_mark = \"✓\" if response_data['is_correct'] else \"✗\"\n",
    "            extracted = response_data['extracted_answer']\n",
    "            correct = response_data['correct_answer']\n",
    "\n",
    "            # Show normalized version too\n",
    "            normalized = normalize_answer(extracted)\n",
    "\n",
    "            # Truncate long answers for display\n",
    "            if len(str(extracted)) > 28:\n",
    "                extracted = str(extracted)[:25] + \"...\"\n",
    "            if len(str(normalized)) > 23:\n",
    "                normalized = str(normalized)[:20] + \"...\"\n",
    "            if len(str(correct)) > 28:\n",
    "                correct = str(correct)[:25] + \"...\"\n",
    "\n",
    "            print(f\"{idx:<8} {model_name:<15} {correct_mark:<8} {extracted:<30} {normalized:<25} {correct:<30}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "def run_evaluation():\n",
    "    print(\"Starting MATH-500 model evaluation...\")\n",
    "    results = evaluate_models()\n",
    "    metrics = calculate_metrics(results)\n",
    "\n",
    "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "    for model_name, data in metrics.items():\n",
    "        print(f\"{model_name}: {data['Overall Accuracy']} (Errors: {data['Error Count']})\")\n",
    "\n",
    "    # Print answer comparison\n",
    "    print_answer_comparison(results)\n",
    "\n",
    "    # Save detailed results to file\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    with open(\"results/math500_detailed_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    with open(\"results/math500_metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # Create a separate answer comparison file\n",
    "    answer_comparison = []\n",
    "    for model_name, data in results.items():\n",
    "        for idx, response_data in enumerate(data['responses']):\n",
    "            answer_comparison.append({\n",
    "                \"problem_idx\": idx,\n",
    "                \"problem\": response_data['problem'][:100] + \"...\" if len(response_data['problem']) > 100 else response_data['problem'],\n",
    "                \"model\": model_name,\n",
    "                \"extracted_answer\": response_data['extracted_answer'],\n",
    "                \"correct_answer\": response_data['correct_answer'],\n",
    "                \"is_correct\": response_data['is_correct']\n",
    "            })\n",
    "\n",
    "    with open(\"results/math500_answer_comparison.json\", \"w\") as f:\n",
    "        json.dump(answer_comparison, f, indent=2)\n",
    "\n",
    "    # Create a more readable CSV version\n",
    "    comparison_df = pd.DataFrame(answer_comparison)\n",
    "    comparison_df.to_csv(\"results/math500_answer_comparison.csv\", index=False)\n",
    "\n",
    "    print(\"Results saved to 'results/' directory.\")\n",
    "\n",
    "    # Create error report\n",
    "    error_summary = {}\n",
    "    for model_name, data in results.items():\n",
    "        error_types = {}\n",
    "        for error in data['errors']:\n",
    "            error_type = error['error_type']\n",
    "            if error_type not in error_types:\n",
    "                error_types[error_type] = 0\n",
    "            error_types[error_type] += 1\n",
    "        error_summary[model_name] = error_types\n",
    "\n",
    "    with open(\"results/math500_error_summary.json\", \"w\") as f:\n",
    "        json.dump(error_summary, f, indent=2)\n",
    "\n",
    "    print(\"\\n===== ERROR SUMMARY =====\")\n",
    "    for model_name, errors in error_summary.items():\n",
    "        print(f\"{model_name} errors:\")\n",
    "        for error_type, count in errors.items():\n",
    "            print(f\"  - {error_type}: {count}\")\n",
    "\n",
    "    comparison_df = pd.DataFrame.from_dict({k: {m: v[m] for m in [\"Overall Accuracy\", \"Error Count\"]}\n",
    "                                           for k, v in metrics.items()}, orient='index').reset_index()\n",
    "    comparison_df.columns = ['Model', 'Overall Accuracy (%)', 'Error Count']\n",
    "    comparison_df.to_csv(\"results/math500_comparison.csv\", index=False)\n",
    "    print(comparison_df)\n",
    "\n",
    "# Run the evaluation\n",
    "run_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
